{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습된 SAC 에이전트 테스트\n",
    "\n",
    "이 노트북은 학습된 스마트팜 제어 모델을 테스트하고 성능을 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 패키지 설치\n",
    "!pip install gymnasium stable-baselines3 numpy matplotlib shimmy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import SAC\n",
    "\n",
    "# 한글 폰트 설정 (Colab용)\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 클래스 정의 (학습할 때와 동일)\n",
    "class SmartFarmEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    스마트팜 환경 시뮬레이터\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SmartFarmEnv, self).__init__()\n",
    "\n",
    "        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(2,), dtype=np.float32)\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0.0, 0.0, 0.0, 0.0], dtype=np.float32),\n",
    "            high=np.array([50.0, 100.0, 50.0, 100.0], dtype=np.float32),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.state = None\n",
    "        self.target_temp = 25.0\n",
    "        self.target_hum = 60.0\n",
    "        self.max_steps = 100\n",
    "        self.current_step = 0\n",
    "\n",
    "        self.ambient_temp = 20.0\n",
    "        self.ambient_hum = 50.0\n",
    "        self.temp_decay = 0.05\n",
    "        self.hum_decay = 0.03\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        start_temp = np.random.uniform(15.0, 35.0)\n",
    "        start_hum = np.random.uniform(40.0, 80.0)\n",
    "\n",
    "        self.state = np.array([start_temp, start_hum, self.target_temp, self.target_hum], dtype=np.float32)\n",
    "        self.current_step = 0\n",
    "\n",
    "        return self.state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        temp_action = np.clip(action[0], -1.0, 1.0)\n",
    "        hum_action = np.clip(action[1], -1.0, 1.0)\n",
    "\n",
    "        current_temp, current_hum, _, _ = self.state\n",
    "\n",
    "        dt_temp = temp_action * 1.5\n",
    "        dt_hum = hum_action * 4.0\n",
    "\n",
    "        dt_temp += (self.ambient_temp - current_temp) * self.temp_decay\n",
    "        dt_hum += (self.ambient_hum - current_hum) * self.hum_decay\n",
    "\n",
    "        noise_temp = np.random.normal(0, 0.2)\n",
    "        noise_hum = np.random.normal(0, 0.8)\n",
    "\n",
    "        next_temp = current_temp + dt_temp + noise_temp\n",
    "        next_hum = current_hum + dt_hum + noise_hum\n",
    "\n",
    "        next_temp = np.clip(next_temp, 0.0, 50.0)\n",
    "        next_hum = np.clip(next_hum, 0.0, 100.0)\n",
    "\n",
    "        self.state = np.array([next_temp, next_hum, self.target_temp, self.target_hum], dtype=np.float32)\n",
    "\n",
    "        temp_error = abs(next_temp - self.target_temp) / 25.0\n",
    "        hum_error = abs(next_hum - self.target_hum) / 50.0\n",
    "\n",
    "        temp_reward = np.exp(-temp_error**2 / 0.1)\n",
    "        hum_reward = np.exp(-hum_error**2 / 0.1)\n",
    "\n",
    "        reward = 0.6 * temp_reward + 0.4 * hum_reward\n",
    "\n",
    "        energy_penalty = 0.01 * (abs(temp_action) + abs(hum_action))\n",
    "        reward -= energy_penalty\n",
    "\n",
    "        self.current_step += 1\n",
    "        terminated = False\n",
    "        truncated = self.current_step >= self.max_steps\n",
    "\n",
    "        return self.state, reward, terminated, truncated, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델 로드\n",
    "print(\"Loading trained model...\")\n",
    "model = SAC.load(\"sac_smartfarm_agent\")\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 단일 에피소드 테스트 (시각화)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 생성\n",
    "env = SmartFarmEnv()\n",
    "\n",
    "# 데이터 저장용 리스트\n",
    "temps = []\n",
    "hums = []\n",
    "temp_actions = []\n",
    "hum_actions = []\n",
    "rewards = []\n",
    "steps = []\n",
    "\n",
    "# 테스트 실행\n",
    "obs, _ = env.reset(seed=42)\n",
    "print(f\"Initial State: Temp={obs[0]:.2f}°C, Humidity={obs[1]:.2f}%\")\n",
    "print(f\"Target: Temp={obs[2]:.2f}°C, Humidity={obs[3]:.2f}%\\n\")\n",
    "\n",
    "for i in range(100):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    \n",
    "    # 데이터 저장\n",
    "    steps.append(i)\n",
    "    temps.append(obs[0])\n",
    "    hums.append(obs[1])\n",
    "    temp_actions.append(action[0])\n",
    "    hum_actions.append(action[1])\n",
    "    rewards.append(reward)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(f\"Test completed: {len(steps)} steps\")\n",
    "print(f\"Average reward: {np.mean(rewards):.3f}\")\n",
    "print(f\"Final Temp: {temps[-1]:.2f}°C (Target: {env.target_temp:.1f}°C)\")\n",
    "print(f\"Final Humidity: {hums[-1]:.2f}% (Target: {env.target_hum:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 10))\n",
    "\n",
    "# 1. 온도 그래프\n",
    "axes[0].plot(steps, temps, label='Current Temperature', color='red', linewidth=2)\n",
    "axes[0].axhline(y=env.target_temp, color='darkred', linestyle='--', label='Target Temperature', linewidth=2)\n",
    "axes[0].fill_between(steps, env.target_temp-2, env.target_temp+2, alpha=0.2, color='red', label='Acceptable Range')\n",
    "axes[0].set_ylabel('Temperature (°C)', fontsize=12)\n",
    "axes[0].set_title('Smart Farm Control Performance - Temperature', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(loc='upper right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. 습도 그래프\n",
    "axes[1].plot(steps, hums, label='Current Humidity', color='blue', linewidth=2)\n",
    "axes[1].axhline(y=env.target_hum, color='darkblue', linestyle='--', label='Target Humidity', linewidth=2)\n",
    "axes[1].fill_between(steps, env.target_hum-5, env.target_hum+5, alpha=0.2, color='blue', label='Acceptable Range')\n",
    "axes[1].set_ylabel('Humidity (%)', fontsize=12)\n",
    "axes[1].set_title('Smart Farm Control Performance - Humidity', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(loc='upper right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. 제어 액션 그래프\n",
    "axes[2].plot(steps, temp_actions, label='Heating/Cooling Action', color='orange', linewidth=2, alpha=0.7)\n",
    "axes[2].plot(steps, hum_actions, label='Humidifier/Dehumidifier Action', color='cyan', linewidth=2, alpha=0.7)\n",
    "axes[2].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[2].set_xlabel('Time Steps', fontsize=12)\n",
    "axes[2].set_ylabel('Action Value', fontsize=12)\n",
    "axes[2].set_title('Control Actions by AI Agent', fontsize=14, fontweight='bold')\n",
    "axes[2].legend(loc='upper right')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].set_ylim(-1.2, 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('smart_farm_test_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGraph saved as 'smart_farm_test_results.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 다중 에피소드 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 에피소드 실행하여 통계 계산\n",
    "n_episodes = 20\n",
    "episode_rewards = []\n",
    "episode_temp_errors = []\n",
    "episode_hum_errors = []\n",
    "episode_convergence_times = []\n",
    "\n",
    "print(f\"Running {n_episodes} test episodes...\\n\")\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    obs, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    temp_errors = []\n",
    "    hum_errors = []\n",
    "    converged_step = None\n",
    "    \n",
    "    for step in range(100):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        \n",
    "        temp_error = abs(obs[0] - env.target_temp)\n",
    "        hum_error = abs(obs[1] - env.target_hum)\n",
    "        \n",
    "        temp_errors.append(temp_error)\n",
    "        hum_errors.append(hum_error)\n",
    "        \n",
    "        # 목표값에 충분히 가까워진 시점 기록\n",
    "        if converged_step is None and temp_error < 2.0 and hum_error < 5.0:\n",
    "            converged_step = step\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_temp_errors.append(np.mean(temp_errors))\n",
    "    episode_hum_errors.append(np.mean(hum_errors))\n",
    "    episode_convergence_times.append(converged_step if converged_step else 100)\n",
    "    \n",
    "    print(f\"Episode {episode+1:2d}: Reward={episode_reward:6.2f}, \"\n",
    "          f\"Avg Temp Error={np.mean(temp_errors):4.2f}°C, \"\n",
    "          f\"Avg Hum Error={np.mean(hum_errors):4.2f}%, \"\n",
    "          f\"Convergence={converged_step if converged_step else 'N/A'} steps\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Average Episode Reward:        {np.mean(episode_rewards):6.2f} ± {np.std(episode_rewards):5.2f}\")\n",
    "print(f\"Average Temperature Error:     {np.mean(episode_temp_errors):6.2f}°C ± {np.std(episode_temp_errors):5.2f}°C\")\n",
    "print(f\"Average Humidity Error:        {np.mean(episode_hum_errors):6.2f}% ± {np.std(episode_hum_errors):5.2f}%\")\n",
    "print(f\"Average Convergence Time:      {np.mean(episode_convergence_times):6.1f} steps\")\n",
    "print(f\"Success Rate (converged):      {sum(1 for t in episode_convergence_times if t < 100)/n_episodes*100:.1f}%\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 통계 시각화\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 에피소드별 보상\n",
    "axes[0, 0].bar(range(1, n_episodes+1), episode_rewards, color='green', alpha=0.7)\n",
    "axes[0, 0].axhline(y=np.mean(episode_rewards), color='red', linestyle='--', \n",
    "                    label=f'Mean: {np.mean(episode_rewards):.2f}', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Total Reward')\n",
    "axes[0, 0].set_title('Episode Rewards', fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 온도 오차\n",
    "axes[0, 1].bar(range(1, n_episodes+1), episode_temp_errors, color='red', alpha=0.7)\n",
    "axes[0, 1].axhline(y=np.mean(episode_temp_errors), color='darkred', linestyle='--', \n",
    "                    label=f'Mean: {np.mean(episode_temp_errors):.2f}°C', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Average Error (°C)')\n",
    "axes[0, 1].set_title('Temperature Control Error', fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 습도 오차\n",
    "axes[1, 0].bar(range(1, n_episodes+1), episode_hum_errors, color='blue', alpha=0.7)\n",
    "axes[1, 0].axhline(y=np.mean(episode_hum_errors), color='darkblue', linestyle='--', \n",
    "                    label=f'Mean: {np.mean(episode_hum_errors):.2f}%', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Episode')\n",
    "axes[1, 0].set_ylabel('Average Error (%)')\n",
    "axes[1, 0].set_title('Humidity Control Error', fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 수렴 시간\n",
    "axes[1, 1].bar(range(1, n_episodes+1), episode_convergence_times, color='purple', alpha=0.7)\n",
    "axes[1, 1].axhline(y=np.mean(episode_convergence_times), color='darkviolet', linestyle='--', \n",
    "                    label=f'Mean: {np.mean(episode_convergence_times):.1f} steps', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Episode')\n",
    "axes[1, 1].set_ylabel('Steps to Convergence')\n",
    "axes[1, 1].set_title('Convergence Speed', fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('performance_statistics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGraph saved as 'performance_statistics.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 다양한 초기 조건 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 극단적인 초기 조건에서 테스트\n",
    "test_conditions = [\n",
    "    {\"name\": \"Very Cold & Dry\", \"temp\": 10.0, \"hum\": 30.0},\n",
    "    {\"name\": \"Very Hot & Humid\", \"temp\": 40.0, \"hum\": 90.0},\n",
    "    {\"name\": \"Cold & Humid\", \"temp\": 15.0, \"hum\": 80.0},\n",
    "    {\"name\": \"Hot & Dry\", \"temp\": 35.0, \"hum\": 40.0},\n",
    "]\n",
    "\n",
    "print(\"Testing under extreme initial conditions...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for condition in test_conditions:\n",
    "    env = SmartFarmEnv()\n",
    "    # 초기 상태를 수동으로 설정\n",
    "    env.state = np.array([condition[\"temp\"], condition[\"hum\"], \n",
    "                          env.target_temp, env.target_hum], dtype=np.float32)\n",
    "    env.current_step = 0\n",
    "    \n",
    "    obs = env.state\n",
    "    total_reward = 0\n",
    "    temp_errors = []\n",
    "    hum_errors = []\n",
    "    \n",
    "    for step in range(100):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        temp_errors.append(abs(obs[0] - env.target_temp))\n",
    "        hum_errors.append(abs(obs[1] - env.target_hum))\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    print(f\"{condition['name']:20s} | Initial: T={condition['temp']:5.1f}°C H={condition['hum']:5.1f}% | \"\n",
    "          f\"Final: T={obs[0]:5.1f}°C H={obs[1]:5.1f}% | \"\n",
    "          f\"Avg Error: T={np.mean(temp_errors):4.2f}°C H={np.mean(hum_errors):4.2f}%\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 랜덤 정책과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAC 모델 vs 랜덤 정책 비교\n",
    "n_test = 10\n",
    "\n",
    "sac_rewards = []\n",
    "random_rewards = []\n",
    "\n",
    "print(\"Comparing SAC agent with random policy...\\n\")\n",
    "\n",
    "for i in range(n_test):\n",
    "    # SAC 모델 테스트\n",
    "    env = SmartFarmEnv()\n",
    "    obs, _ = env.reset()\n",
    "    sac_reward = 0\n",
    "    \n",
    "    for _ in range(100):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        sac_reward += reward\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    sac_rewards.append(sac_reward)\n",
    "    \n",
    "    # 랜덤 정책 테스트\n",
    "    env = SmartFarmEnv()\n",
    "    obs, _ = env.reset()\n",
    "    random_reward = 0\n",
    "    \n",
    "    for _ in range(100):\n",
    "        action = env.action_space.sample()  # 랜덤 행동\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        random_reward += reward\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    random_rewards.append(random_reward)\n",
    "\n",
    "# 결과 비교\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(2)\n",
    "means = [np.mean(sac_rewards), np.mean(random_rewards)]\n",
    "stds = [np.std(sac_rewards), np.std(random_rewards)]\n",
    "colors = ['green', 'gray']\n",
    "\n",
    "bars = ax.bar(x, means, yerr=stds, color=colors, alpha=0.7, capsize=10)\n",
    "ax.set_ylabel('Average Total Reward', fontsize=12)\n",
    "ax.set_title('SAC Agent vs Random Policy Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['SAC Agent (Trained)', 'Random Policy'])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 값 표시\n",
    "for i, (mean, std) in enumerate(zip(means, stds)):\n",
    "    ax.text(i, mean + std + 1, f'{mean:.2f}\\n±{std:.2f}', \n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sac_vs_random.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSAC Agent:     {np.mean(sac_rewards):6.2f} ± {np.std(sac_rewards):5.2f}\")\n",
    "print(f\"Random Policy: {np.mean(random_rewards):6.2f} ± {np.std(random_rewards):5.2f}\")\n",
    "print(f\"\\nImprovement: {((np.mean(sac_rewards) - np.mean(random_rewards)) / abs(np.mean(random_rewards)) * 100):.1f}%\")\n",
    "print(\"\\nGraph saved as 'sac_vs_random.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 실시간 인터랙티브 테스트 (목표값 변경)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 목표 온도/습도를 중간에 변경하여 적응력 테스트\n",
    "env = SmartFarmEnv()\n",
    "obs, _ = env.reset()\n",
    "\n",
    "temps = []\n",
    "hums = []\n",
    "target_temps = []\n",
    "target_hums = []\n",
    "steps_list = []\n",
    "\n",
    "print(\"Testing adaptability to changing target values...\\n\")\n",
    "\n",
    "# 목표값 변경 시나리오\n",
    "target_changes = [\n",
    "    (0, 25.0, 60.0),    # 초기 목표\n",
    "    (30, 28.0, 70.0),   # 30스텝에서 변경\n",
    "    (60, 22.0, 50.0),   # 60스텝에서 변경\n",
    "]\n",
    "\n",
    "change_idx = 0\n",
    "for step in range(100):\n",
    "    # 목표값 변경\n",
    "    if change_idx < len(target_changes) and step == target_changes[change_idx][0]:\n",
    "        env.target_temp = target_changes[change_idx][1]\n",
    "        env.target_hum = target_changes[change_idx][2]\n",
    "        env.state[2] = env.target_temp\n",
    "        env.state[3] = env.target_hum\n",
    "        print(f\"Step {step}: Target changed to T={env.target_temp:.1f}°C, H={env.target_hum:.1f}%\")\n",
    "        change_idx += 1\n",
    "    \n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    \n",
    "    steps_list.append(step)\n",
    "    temps.append(obs[0])\n",
    "    hums.append(obs[1])\n",
    "    target_temps.append(env.target_temp)\n",
    "    target_hums.append(env.target_hum)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "# 시각화\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# 온도\n",
    "axes[0].plot(steps_list, temps, label='Actual Temperature', color='red', linewidth=2)\n",
    "axes[0].plot(steps_list, target_temps, label='Target Temperature', color='darkred', \n",
    "             linestyle='--', linewidth=2)\n",
    "for change in target_changes[1:]:\n",
    "    axes[0].axvline(x=change[0], color='gray', linestyle=':', alpha=0.5)\n",
    "axes[0].set_ylabel('Temperature (°C)', fontsize=12)\n",
    "axes[0].set_title('Adaptability Test - Temperature Tracking', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 습도\n",
    "axes[1].plot(steps_list, hums, label='Actual Humidity', color='blue', linewidth=2)\n",
    "axes[1].plot(steps_list, target_hums, label='Target Humidity', color='darkblue', \n",
    "             linestyle='--', linewidth=2)\n",
    "for change in target_changes[1:]:\n",
    "    axes[1].axvline(x=change[0], color='gray', linestyle=':', alpha=0.5)\n",
    "axes[1].set_xlabel('Time Steps', fontsize=12)\n",
    "axes[1].set_ylabel('Humidity (%)', fontsize=12)\n",
    "axes[1].set_title('Adaptability Test - Humidity Tracking', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('adaptability_test.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAdaptability test completed!\")\n",
    "print(\"Graph saved as 'adaptability_test.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결론\n",
    "\n",
    "이 테스트를 통해 다음을 확인할 수 있습니다:\n",
    "\n",
    "1. **제어 정확도**: 목표 온도/습도에 얼마나 가까이 유지하는가\n",
    "2. **수렴 속도**: 목표값에 도달하는데 걸리는 시간\n",
    "3. **안정성**: 다양한 초기 조건에서도 안정적으로 작동하는가\n",
    "4. **랜덤 정책 대비 개선도**: 학습의 효과\n",
    "5. **적응력**: 목표값이 변경될 때 빠르게 적응하는가"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
