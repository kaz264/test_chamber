{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea9069c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.11' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/Python310-32/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "pip install gymnasium stable-baselines3 numpy shimmy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16ddaf5",
   "metadata": {},
   "outputs": [],
   "source": "import gymnasium as gym\nimport numpy as np\nfrom gymnasium import spaces\nfrom stable_baselines3 import SAC\n\n# ==========================================\n# 1. 가상 스마트팜 환경 (Digital Twin) 정의\n# ==========================================\nclass SmartFarmEnv(gym.Env):\n    \"\"\"\n    [문서의 Digital Twin 역할]\n    - 상태(State): [현재온도, 현재습도, 목표온도, 목표습도]\n    - 행동(Action): [냉난방기 출력(-1~1), 가습/제습기 출력(-1~1)]\n    - 보상(Reward): 목표치와의 오차가 작을수록 점수 높음 (Scalarization)\n    \"\"\"\n    def __init__(self):\n        super(SmartFarmEnv, self).__init__()\n\n        # 행동 공간 정의:\n        # Action 0: 히터/에어컨 (-1: 풀냉방, 0: 꺼짐, 1: 풀난방)\n        # Action 1: 가습/제습 (-1: 풀제습, 0: 꺼짐, 1: 풀가습)\n        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(2,), dtype=np.float32)\n\n        # 관찰 공간 정의: [현재온도, 현재습도, 목표온도, 목표습도]\n        # 온도는 0~50도, 습도는 0~100% 범위로 가정\n        self.observation_space = spaces.Box(\n            low=np.array([0.0, 0.0, 0.0, 0.0], dtype=np.float32),\n            high=np.array([50.0, 100.0, 50.0, 100.0], dtype=np.float32),\n            dtype=np.float32\n        )\n\n        # 초기 상태 설정\n        self.state = None\n        self.target_temp = 25.0\n        self.target_hum = 60.0\n        self.max_steps = 100 # 한 에피소드 길이 (모의 농사 기간)\n        self.current_step = 0\n\n        # 물리 상수\n        self.ambient_temp = 20.0  # 외부 온도 (실온으로 복귀하려는 경향)\n        self.ambient_hum = 50.0   # 외부 습도\n        self.temp_decay = 0.05    # 실온 복귀 계수\n        self.hum_decay = 0.03     # 습도 복귀 계수\n\n    def reset(self, seed=None, options=None):\n        # 환경 초기화 (농사 시작)\n        super().reset(seed=seed)\n\n        # 랜덤하게 시작 온도/습도 설정 (도메인 랜덤화 효과)\n        start_temp = np.random.uniform(15.0, 35.0)\n        start_hum = np.random.uniform(40.0, 80.0)\n\n        self.state = np.array([start_temp, start_hum, self.target_temp, self.target_hum], dtype=np.float32)\n        self.current_step = 0\n\n        return self.state, {}\n\n    def step(self, action):\n        # [문서의 물리 엔진 계산 단계]\n        temp_action = np.clip(action[0], -1.0, 1.0) # 냉난방 출력\n        hum_action = np.clip(action[1], -1.0, 1.0)  # 가습제습 출력\n\n        current_temp, current_hum, _, _ = self.state\n\n        # --- 물리 법칙 시뮬레이션 (개선된 모델) ---\n        # 1. 행동에 따른 변화\n        dt_temp = temp_action * 1.5  # 냉난방 효과\n        dt_hum = hum_action * 4.0    # 가습/제습 효과\n\n        # 2. 자연적인 복귀력 (외부 환경으로 돌아가려는 경향)\n        dt_temp += (self.ambient_temp - current_temp) * self.temp_decay\n        dt_hum += (self.ambient_hum - current_hum) * self.hum_decay\n\n        # 3. 환경 노이즈 (측정 오차 및 외란)\n        noise_temp = np.random.normal(0, 0.2)\n        noise_hum = np.random.normal(0, 0.8)\n\n        # 상태 업데이트 (Next State)\n        next_temp = current_temp + dt_temp + noise_temp\n        next_hum = current_hum + dt_hum + noise_hum\n\n        # 범위 제한 (Clipping)\n        next_temp = np.clip(next_temp, 0.0, 50.0)\n        next_hum = np.clip(next_hum, 0.0, 100.0)\n\n        self.state = np.array([next_temp, next_hum, self.target_temp, self.target_hum], dtype=np.float32)\n\n        # --- 보상 계산 (Reward Calculation: 정규화된 오차) ---\n        # 목표와의 오차(Error) 계산 및 정규화\n        temp_error = abs(next_temp - self.target_temp) / 25.0  # 온도 오차를 0~1로 정규화 (최대 ±25도 가정)\n        hum_error = abs(next_hum - self.target_hum) / 50.0     # 습도 오차를 0~1로 정규화 (최대 ±50% 가정)\n\n        # 가우시안 보상 함수 (목표에 가까울수록 높은 보상)\n        temp_reward = np.exp(-temp_error**2 / 0.1)  # 온도 보상 (0~1)\n        hum_reward = np.exp(-hum_error**2 / 0.1)    # 습도 보상 (0~1)\n\n        # 최종 보상: 가중 평균 (온도를 더 중요하게)\n        reward = 0.6 * temp_reward + 0.4 * hum_reward\n\n        # 에너지 소비 페널티 (지속가능성 고려)\n        energy_penalty = 0.01 * (abs(temp_action) + abs(hum_action))\n        reward -= energy_penalty\n\n        # 종료 조건\n        self.current_step += 1\n        terminated = False\n        truncated = self.current_step >= self.max_steps\n\n        return self.state, reward, terminated, truncated, {}\n\n# ==========================================\n# 2. SAC 에이전트 학습 (Training Loop)\n# ==========================================\n\n# 1) 환경 생성\nenv = SmartFarmEnv()\n\n# 2) SAC 모델 초기화\n# - Policy: \"MlpPolicy\" (이미지를 안 쓰므로 MLP 사용)\n# - ent_coef=\"auto\": 문서에 나온 Entropy Maximization 자동 조절\nmodel = SAC(\n    \"MlpPolicy\",\n    env,\n    verbose=1,\n    learning_rate=3e-4,\n    buffer_size=50000,  # Replay Buffer 크기 (충분한 경험 저장)\n    batch_size=256,\n    learning_starts=1000,  # 학습 시작 전 수집할 경험의 양\n    tau=0.005,             # Soft update 계수 (타겟 네트워크)\n    gamma=0.99,            # 할인 인자 (미래 보상 중요도)\n    ent_coef='auto',       # 엔트로피 자동 조정 (탐험 vs 이용 균형)\n    target_update_interval=1,\n    train_freq=1\n)\n\nprint(\"--- 학습 시작 (농사 시뮬레이션 진행 중...) ---\")\n# 3) 학습 실행 (50,000 스텝으로 증가)\nmodel.learn(total_timesteps=50000, log_interval=10)\nprint(\"--- 학습 완료 ---\")\n\n# ==========================================\n# 3. 학습된 모델 테스트 (Inference)\n# ==========================================\nprint(\"\\n--- 실제 제어 테스트 ---\")\nobs, _ = env.reset()\ntotal_reward = 0\n\nfor i in range(20):\n    # 학습된 모델이 행동 결정 (deterministic=True는 탐험 끄고 정석대로 하라는 뜻)\n    action, _state = model.predict(obs, deterministic=True)\n\n    # 환경에 행동 적용\n    obs, reward, terminated, truncated, _ = env.step(action)\n    total_reward += reward\n\n    current_temp = obs[0]\n    current_hum = obs[1]\n    target_temp = obs[2]\n    target_hum = obs[3]\n\n    print(f\"Step {i+1}: 온도={current_temp:.2f}°C (목표: {target_temp:.1f}°C), \"\n          f\"습도={current_hum:.2f}% (목표: {target_hum:.1f}%) | \"\n          f\"행동: [난방 {action[0]:+.2f}, 습도 {action[1]:+.2f}] | 보상: {reward:.3f}\")\n\n    if terminated or truncated:\n        break\n\nprint(f\"\\n평균 보상: {total_reward / (i+1):.3f}\")\n\n# 모델 저장\nmodel.save(\"sac_smartfarm_agent\")\nprint(\"\\n모델이 'sac_smartfarm_agent.zip'로 저장되었습니다.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}